# config.yaml
problem_name: cap_set
initial_program_path: initial_program.py
evaluator_script_path: evaluator.py
language: python
goal: maximize
max_iterations: 10 # <-- SET TO 10 FOR DEBUGGING

# LLM configuration
llm:
  primary_model: "gpt-oss:120b-cloud"
  api_base: "http://localhost:11434/v1"
  temperature: 0.8 # Slightly higher temp
  max_tokens: 16000
  timeout: 120

# Prompt configuration
prompt:
  system_message: "You are optimizing a priority function for cap set construction. The function `priority_function(v, n)` takes a vector `v` (tuple of 0,1,2 values) and dimension `n`, returning a float priority. Higher priority vectors are selected first. Modify ONLY the priority_function to find larger cap sets. Return only the improved function code."


# Database configuration
database:
  population_size: 50
  archive_size: 20
  num_islands: 3
  elite_selection_ratio: 0.2
  exploitation_ratio: 0.7
  embedding_model: "text-embedding-3-small"
  similarity_threshold: 0.99

# Evaluator configuration
evaluator:
  timeout: 60 # Might need longer timeout for n=5 brute force? Keep for now.
  cascade_evaluation: false
  cascade_thresholds: []
  parallel_evaluations: 3

# Evolution settings
diff_based_evolution: false # Use full rewrite
max_code_length: 20000

# Novelty Judge configuration
novelty_judge:
  use_llm_novelty_check: false

# Add this section to your config.yaml
logging:
  level: DEBUG
  log_llm_requests: true
  log_llm_responses: true
  log_file: "llm_debug.log"
